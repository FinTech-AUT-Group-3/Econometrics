---
title: |
  | Financial Econometrics in R/Python
  | Group Assignment 2
author: |
  | Group 3
  | The Business School, Imperial College London
  | Rakesh Bali (CID: 02478023)
  | Rui Moreno (CID: 02532521)
  | Siyu Liu (CID: 02479121)
  | Zhiyu Chen (CID: 02517659)
  | Zhuofan Chen (CID: 02151338)
date: "18-11-2023"
output: pdf_document
---

```{=tex}
\newpage
\tableofcontents
```
\newpage

## Preparation of R and Data

### Loading R packages

Load the packages required.

```{r loadLibs, message=FALSE, warning=FALSE}
library(quantmod)
library(dplyr)
library(readxl)
library(moments)
library(lmtest)
library(sandwich) 
library(MASS)
library(ggplot2)
library(knitr)
library(margins)
library(jtools)
library(e1071) # For Naive Bayesian Classifier in (h)
library(tree) # For dicision tree in (h)
library(randomForest) # For random Forest in (h)
library(class) # For k-NN in (h)
```

### Loading Data

Load `growthdata.xlsx` into a data frame.

```{r load data, message=FALSE, warning=FALSE}
employment_data <- read_excel("employment_08_09.xlsx")
# clean data
data <- na.omit(employment_data)
```

\newpage

## Question a

*What fraction of workers in the sample were employed in April 2009? Use your answer to compute a 95% confidence interval for the probability that a worker was employed in April 2009, conditional on being employed in April 2008*

```{r}
number_of_workers_employed_2008 <- length(employment_data$employed)

workers_employed_2009 <- employment_data %>% filter(employed == 1)

number_of_workers_employed_2009 <-
  length(workers_employed_2009$employed)

fraction_of_workers_employed_2009 <-
  round(number_of_workers_employed_2009 / number_of_workers_employed_2008,
        4)

standard_error_fraction_employed_2009 <-
  sqrt((
    fraction_of_workers_employed_2009 * (1 - fraction_of_workers_employed_2009)
  ) / number_of_workers_employed_2008)

confidence_level <- 0.95
margin_of_error <-
  qnorm((1 + confidence_level) / 2) * standard_error_fraction_employed_2009

confidence_interval <-
  c(
    fraction_of_workers_employed_2009 - margin_of_error,
    fraction_of_workers_employed_2009 + margin_of_error
  )

print(confidence_interval)
```

In April 2009, 87.55% of workers were employed. On a 95% confidence level, the confidence interval is computed for the probability that a worker was employed in April 2009 to be [0.867, 0.884].

\newpage

## Question b

*Regress Employed on Age and Age2 , using a linear probability model*

```{r question b linear probability regression on age and squared age}
# required linear probability model
lpm_b <-  lm(employed ~ age + I(`age`^2), data = data)
summary(lpm_b)
```

### Question b-i)

*Based on this regression, was the age a statistically significant determinant of employment in April 2009*

```{r}
# t test - statistical significance of coefficients
coeftest(lpm_b, vcov = vcovHC(lpm_b), type = "HC1")
```

The *age* variable is a statistically significant determinant of 2009 April Employment rate. The linear probability model with *age* and squared *age* regressed on *employed* gives a relationship of $$ employed = 0.3218 + 0.02746 \times age - 0.0003159 \times age^2$$.

The t-value of the *age* variable at 9.491 is significant enough to reject the null hypothesis at a significance level of 1%, i.e. there is no statistical evidence that the coefficient of *age* in the probability model is different from zero. The p-value being less than $$2 \times 10^{-16}$$, which is way smaller than the significance level, further supports that *age* is a statically significant predictor of *employed*. This variable stays significant even after adjustment for hetero skedasticity with a t-vale of 7.9662.

### Question b-ii)

*Is there evidence of a nonlinear effect of age on probability of being employed?*

The above linear probability model indicates a inverted parabola in the relationship between *squared age* and employment rate in April 2009. Any one unit of age rise is expected to decrease the probability of being employed by $$3.159 \times 10^{-4}$$ unit. A p-value of less than $$2 \times 10^{-16}$$ of the *squared age* variable suggests there is strong evidence against the null hypothesis that the true coefficient of *squared age* is zero. With a statistically significant t-value of -9.08, the quadratic term is proved to be contributing to the model and thereon a non-linearity of *age* on the probability of being employed is detected.

In order to further prove the non-linearity, i.e. to prove the significance of the squared term of *age*, a linear probability model *lpm_b_1* with *age* as the only regressor is run below. With a much higher F-statistics at 47.59, the main regression *lpm_b* in question b-i carries better overall fitting goodness of the dependent variable *employed*, and *squared age* thereon should be included in a linear regression to model it. 
```{r}
# regression to detect the linearity of the variable 'age' only 
lpm_b_1 <- lm(employed ~ age, data = data)
waldtest(lpm_b, lpm_b_1)
```
Taking the main regression *lpm_b* as the fitted objective, a Wald test has been performed on it against the hypothesis matrix of *lpm_b_1* regression coefficients. The null hypothesis that coefficients of these two models are the same can be rejected at significance level of 1% with a F-statistics of 82.439. Therefore, *squared age* is of statistically significant explanatory power over *employed* due to the fact that including it in the linear probability model will cause a material difference to the regression. The importance of the squared term proves against the linearity of *age*.

### Question b-iii)

*Compute the predicted probability of employment for a 20-year-old worker, a 40year-old worker, and a 60-year-old worker*

```{r}
# predictions for sets of values of three models above
new_data <- data.frame(age = c(20, 40, 60),
                       age_sqr = c(400, 1600, 3600))
lpm_prediction <- predict(lpm_b, new_data, type = "response")
names(lpm_prediction) <- c("Age 20", "Age 40", "Age 60")
print(list(lpm_prediction = lpm_prediction))
```
Based on the linear probability model, a new data set is created given the specified values of the predictors *age* and *squared age*. The prediction list states the estimated probabilities of being employed at the age of 20, 40 and 60 are approximately 74.47%, 91.48% and 83.22% respectively under the assumption of a linear relationship of both the linear and the quadratic forms of *age* on *employed*. 

```{r}
age <- seq(18, 63)

ggplot(data, aes(x=age, fill= as.factor(employed))) + 
  geom_bar(position = "dodge", stat = "count") +   
  labs(title = "Employment Status by Age", x = "Age",y = "Count")
```
A sample *age* sequence from 18 to 63 was further created in order to detect the movement of *employed*. The *unemployed* variable stays within a consistent range across different age levels. The increasing trend of the employment probability, on the other hand, inverted in the *age* range around 40 to 50 to decrease against age level.

This is consistent with what happens in the real world after an economic recession. Younger individuals often have less work experience and may be employed in entry-level positions that are more susceptible to economic fluctuations. Individuals in the middle-age bracket may have accumulated a moderate level of experience, potentially making them more resilient to job losses during economic downturns. Job security for older individuals decreases post-recession, possibly due to factors such as industry restructuring, technological advancements, or age-related biases.

```{r}
a = lpm_b$coefficients[3]
b = lpm_b$coefficients[2]
c = lpm_b$coefficients[1]
f = function(x) {
  a * x ^ 2 + b * x + c
}

plot(age, f(age), type = 'l', ylab = "probability of being employed")

print(paste("The age with the highest probability of being employed in 2009 is",
            round(- b / (2 * a), 1)))
```
The above curve suggests that the probability of being employed is lower at the extremes of *age*, both young and old, and higher in the middle range. The peak of the employment probability appears at the age of 43.5. The U-shape of the curve again emphasized non-linearity of the linear probability model as the increasing trend did not continue after the age of 43.5 but turned to decrease instead, which is a characteristic of quadratic relatioship.

\newpage

## Question c

*Repeat (b) using a probit regression*

```{r question c probit regression on age and squared age}
# required linear probability model
probit_c <- glm(employed ~ age + I(`age` ^ 2), 
                family = binomial(link = "probit"), data)
summary(probit_c)
```

### Question c-i)

*Based on this regression, was the age a statistically significant determinant of employment in April 2009*

```{r}
# t test - statistical significance of coefficients
coeftest(probit_c, vcov = vcovHC(probit_c), type = "HC1")
```

The p-value of the coefficient of age is less than $$2 \times 10^{-16}$$, which is close enough to zero. At a significance level of 1% the null hypothesis can be rejected. Thus, based on our probit regression, it can be concluded that *age* is a statistically significant determinant of *employed* in April 2009.

```{r}
margins(probit_c)
```

The z-value of the *age* variable at 8.555 is further evidence to reject the null hypothesis at a significance level of 1%, i.e. there is no statistical evidence that the coefficient of *age* in the probability model is different from zero. 

### Question c-ii)

*Is there evidence of a nonlinear effect of age on the probability of being employed?*

The p-value of the coefficient of *squared age* is 4.748e-16, which is nearly zero as well. As it is less than our significance level, we could reject the null hypothesis. Thus, based on our probit regression, we could say *squared age* is also a statistically significant determinant of employment in April 2009. The quadratic term is proved to be contributing to the model and therefore a non-linearity of *age* on the probability of being employed is detected.

In order to further prove the non-linearity, i.e. to prove the significance of the squared term of *age*, a probit model with *age* as the only regressor is run below. And then waldest is performed on both the models i.e. one model with *squared age* and the other without. The result of the waldtest confirms that both the models are different at less than 1% significance. Hence, it is confirmed that the *squared age* is significant and it explains the nonlinear effect of age on the probability of being employed. 

```{r}
# regression to detect the linearity of the variable 'age' only
probit_c_1 <- glm(employed ~ age, 
                  family = binomial(link = "probit"), data)
waldtest(probit_c, probit_c_1)
```
By looking at the Wald test between the LPM model and the Probit model,a p-value of less than 2.2*10^(-16) is observed, which is nearly zero. As p-value is less than the significance level (even considering a 1% here), the null hypothesis could be rejected which states that the two models are the same. Therefore, the probit model is different from the LPM model, and the non-linearity of *age* on the probability of the probability being employed is reinforced.


### Question c-iii)

*Compute the predicted probability of employment for a 20-year-old worker, a 40year-old worker, and a 60-year-old worker*

```{r}
# predictions for sets of values of three models above
new_data <- data.frame(age = c(20, 40, 60),
                       age_sqr = c(400, 1600, 3600))
probit_prediction <- predict(probit_c, new_data, type = "response")
names(probit_prediction) <- c("Age 20", "Age 40", "Age 60")
print(list(probit_prediction = probit_prediction))
```


```{r}
a = probit_c$coefficients[3]
b = probit_c$coefficients[2]
c = probit_c$coefficients[1]
f = function(x) {
  pnorm(a * x^2 + b * x + c)
}
age <- seq(15, 75)
plot(age, f(age), type = 'l', ylab = "probability of being employed")

print(paste("The age with the highest probability of being employed in 2009 is",
            round(- b / (2 * a), 1)))
```
Based on the probit model, a new data set is created given the specified values of the predictors *age* and *squared age*. The prediction list states the estimated probabilities of being employed at the age of 20, 40 and 60 are approximately 73.27%, 91.08% and 86.63% respectively. This means, for example, a 20-year-old worker is predicted to be employed in April 2009 with a probability of 0.7327351, based on the historical data we have. 

The lowest predicted employment probability shows in an *age* of 20 might be due to the fact that people at a younger age of 20 have less experience and thereon face greater exposure to unemployment risk at economic recession. And the old people may face some physical restrictions while working, which may also have a significant impact on their chance of getting employed. While the middle-age people are the main workforce in society with their better physical and experiential conditions. Such trend can also be shown in our plot of probability of being employed under probit model.

\newpage

## Question d

*Repeat (b) using a logit regression*

```{r question c logit regression on age and squared age}
# required linear probability model
logit_d <- glm(employed ~ age + I(`age` ^ 2), 
               family = binomial(link = "logit"), data)
summary(logit_d)
```

### Question d-i)

*Based on this regression, was the age a statistically significant determinant of employment in April 2009*

```{r}
# t test - statistical significance of coefficients
coeftest(logit_d, vcov = vcovHC(logit_d), type = "HC1")
```
The logistic regression is a statistical method used for modeling the probability of a dependent variable is categorical with two levels at 0 and 1, in this case the *employed* binary variable. The sigmoid function is used to model the relationship of *age* and *squared age* against the employment probability.

Based on the above result, the *age* variable is a statistically significant determinant of April 2009 Employment probability rate with over 99% confidence level as the model p-value is less than 0.01. One-unit change in *age* shall lead to 0.2187 units of change in the log-odds of the corresponding dependent variable *employed*.

```{r}
margins(logit_d)
```

The z-value of the *age* variable at 8.7968 is significant enough to reject the null hypothesis at a significance level of 1%, i.e. there is no statistical evidence that the coefficient of *age* in the probability model is different from zero. The p-value being less than $$2 \times 10^{-16}$$, which is way smaller than the significance level, further supports that *age* is a statically significant predictor of *employed*.

### Question d-ii)

*Is there evidence of a nonlinear effect of age on probability of being employed?*

The logit model *logid_d* above reveals a distinctive concave shape in the relationship between *squared age* and *employed* in April 2009. The statistical significance of the t-value at -8.3235 associated with the quadratic term validates its substantial impact on the model, indicating a pronounced non-linear effect of *age* on the likelihood of being employed.

To further establish the significance of the non-linear relationship, a logit model *logit_d_1* with *age* as the only regressor is created below. Subsequently, a Wald test is conducted on both models: one incorporating the *squared age* term and the other without. The results of the Wald test provide compelling evidence that the two models differ significantly at a significance level of less than 1%. This again confirms the importance of the *squared age* term, substantiating its role in explaining the non-linear impact of *age* on the probability of being employed.

```{r}
# regression to detect the linearity of the variable 'age' only
logit_d_1 <- glm(employed ~ age, 
                 family = binomial(link = "logit"), data)
waldtest(logit_d, logit_d_1)
```

### Question d-iii)

*Compute the predicted probability of employment for a 20-year-old worker, a 40year-old worker, and a 60-year-old worker*

```{r}
# predictions for sets of values of three models above
new_data <- data.frame(age = c(20, 40, 60),
                       age_sqr = c(400, 1600, 3600))
logit_prediction <- predict(logit_d, new_data, type = "response")
names(logit_prediction) <- c("Age 20", "Age 40", "Age 60")
print(list(logit_prediction = logit_prediction))
```

Based on the *logit_d* model, a new data set is created given the specified values of the predictors *age* and *squared age*. With another similar tendency, the logit prediction list states the estimated probabilities of being employed at the age of 20, 40 and 60 are approximately 72.86%, 91.06% and 83.58% respectively.

```{r}
a = logit_d$coefficients[3]
b = logit_d$coefficients[2]
c = logit_d$coefficients[1]
f = function(x) {
  1 / (1 + exp(-a * x ^ 2 - b * x - c))
}
age <- seq(15, 75)
plot(age, f(age), type = 'l', ylab = "probability of being employed")

print(paste("The age with the highest probability of being employed in 2009 is", round(- b / (2 * a), 1)))
```

Again the sample *age* sequence from 18 to 63 was created to plot the employment probability curve with a similar depicted curve tendency and a similar *age* peak at 43.2 years old. Beyond this peak point, the rising employment probability starts to decline, illustrating a characteristic quadratic relationship. The value of *employed* diminishes at both tails to underscore the non-linear nature of the logistic regression model.

\newpage

## Question e

*Are there important differences in your answers to (b)-(d)? Explain*

```{r}
model_list <-
  list(summary(margins(lpm_b)), summary(margins(probit_c)), summary(margins(logit_d)))

model_df <- do.call(rbind, model_list)
model_names <- c("LPM", "Probit", "Logit")

model_df <- cbind(Model = model_names, model_df)
model_df
```
The probabilities of being employed are 0.089%, 0.11%, and 0.12% observed from the Linear Probability Model (LPM),  Probit, Logit models, respectively. Thus, there are not much difference from the answers to question (b) to (d).
However, these three models are very different in definition. LPM is a regression model used for binary outcomes, where the dependent variable is binary (usually 0 or 1). Coefficients in LPM can be interpreted as the change in the probability of the event for a one-unit change in the corresponding independent variable. However, there is a limitation of LPM - the predicted probabilities can fall outside the usual probability [0, 1] range, and it may suffer from heteroscedasticity.

Probit is a type of regression model used for binary outcomes, similar to the LPM. However, it uses the cumulative distribution function of the standard normal distribution (probit function) to model the relationship. Unlike LPM, coefficients in the probit model represent the change in the z-score of the latent variable for a one-unit change in the corresponding independent variable.

Consider a Logit model where *y* is the dependent variable and the *x*s are the independent variables, the log odds of the probabilities (log(P(x)/1-P(x)) is modeled as a linear combination of the predictor variables. Different from LPM, increasing the i-th *x* by one unit changes the log odds by units of the amount of the corresponding coefficient *beta-i*. Equivalently, the odds change in percentage by exponential of *beta-i*. Both Probit and Logic models will have the predicted probabilities lying in the usual probability range of [0,1]. 


Regarding to the limitations of Probit and Logit model, they are both logistic regression models and have a non-linear relationship between the independent variables and the probability of the event occurring. While this is often more realistic than the linear probability model, it can make interpretation more complex.

Thus, the three models are different from each other by definition, even though they show similar results in our analysis of the effect of age on employment in question (b) to (d).


\newpage

The data set includes variables measuring the workers’ educational attainment, sex, race, marital
status, region of the country, and weekly earnings in April 2008.

## Question f

```{r}
lpm_f <- lm(employed ~ age + I(age^2) + educ_lths + educ_hs + educ_somecol + 
              educ_aa + educ_bac + educ_adv + female + married + race + 
              ne_states + so_states + ce_states + we_states + earnwke, 
            data = data)
summary(lpm_b)
summary(lpm_f)
```

### Question f-i)

*By adding those covariates to the linear probability model regression of point (b), investigate whether the conclusions on the effect of Age on employment from (b) are affected by omitted variable bias*

The comparison between the initial linear regression model and the adjusted model reveals a small difference in the coefficients for *age* and *squared age*. In the initial model *lpm_b*, *age* and *squared age* exhibited significant effects in different directions on employment probability. In the adjusted model *lpm_f*, while the coefficients for *age* and *squared age* remain significant at a significance level of 1% with t-values at 8.087 and -7.954 respectively, the coefficient values have changed slightly. This difference suggests that the additional variables have impacted the interpretation of the *age*-related effects on *employed*. Namely, the introduction of new variables such as regional factors, weekly earnings and *educ_lths* - the highest level of education is less than a high school graduate - points out potential omitted variable bias in the initial model.

Additionally, the statistical significance of certain new variables including educational factors, regional factors and weekly earnings in the adjusted model highlights the importance of considering a broader set of factors. Moreover, the higher adjusted R-squared of the adjusted model indicates an increase in explanatory power which in turn suggests the added co-variates contribute to a better understanding of the variability in employment status.

To conclude, while these results might suggest the presence of some omitted variable bias in the initial model, the small coefficient difference for *age* and *squared age* between the two models and the fact they are still extremely significant in the adjusted model leads us to think there is not an impactful omitted variable bias. Nevertheless, it is important to point out that the adjusted model better explains the changes due to the added covariates

### Question f-ii)

*Use the regression results to discuss the characteristics of workers who were hurt the most by the 2008 financial crisis*

The regression results provide insights into the characteristics of workers particularly affected by the 2008 financial crisis.

Firstly, the negative coefficient for the variable representing workers with less than a high school education - *educ_lths* - suggests that individuals with lower educational attainment faced a more significantly negative impact on their employment status, i.e. they were more likely to be unemployed in April 2009. This fact aligns with the usual economic literature which defends a higher vulnerability of less-educated workers during economic downturns.

Additionally, the positive coefficient for the variable indicating workers from central states - *ce_states* - implies that individuals residing in central regions of the country experienced a more significantly negative effect on their employment probability compared to those in other regions. One possible explanation for the geographical disparity in the impact of the financial crisis could be attributed to variations in industry composition across different regions. This aligns with the fact that central states are deeply connected with the automotive industry (e.g. Michigan) and with manufacturing and exports, both of which were severely affected by the Great Recession.

Moreover, the positive and statistically significant coefficient for weekly earnings - *earnwke* - indicates that workers with higher pre-crisis earnings experienced a relatively less severe negative impact on their employment status. This underscores the financial resilience of individuals with higher earnings, suggesting that economic downturns disproportionately affected those with lower income levels. One possible explanation is the tendency of large corporate companies to heavily downsize their workforce during a financial crisis to reduce costs but not its higher levels - or at least not to the same extent. Subsequently, this leads to the correlation between lesser weekly earnings - *workforce* - and a higher probability of being let go during a financial crisis. Another possible explanation comes from the fact higher weekly earners are usually more qualified. For example, if we considered someone who became unemployed after the data was collected, the more qualified they were the higher the chance they could find another position before April 2009. In turn, this would again substantiate the positive coefficient for weekly earnings.

Finally, the positive coefficient for age suggests that, on average, older workers experienced more favourable outcomes regarding employment in the aftermath of the crisis. This finding is consistent with the idea that older workers possess valuable experience, skills, and industry knowledge, making them more valuable to a company and, consequently, more resilient to economic downturns. However, the negative coefficient for *squared age* indicates a diminishing positive effect with age increase and a decline in this effect for the oldest workers. This diminishing effect could be due to factors such as age-related discrimination, and changing labour market dynamics such as technological changes or retirement, all of which would be exacerbated during a financial crisis and associated cost-cutting policies.

Overall, the analysis suggests that the 2008 financial crisis disproportionately affected younger workers, those with lower educational attainment and less weekly earnings, and those residing in specific geographical regions.

\newpage

## Question (g)

```{r}
# Predictions for models b, c, d (In-sample)
predictions_b_raw <- predict(lpm_b, data, type = "response")  # For linear probability model
predictions_c_raw <- predict(probit_c, data, type = "response")  # For probit model
predictions_d_raw <- predict(logit_d, data, type = "response")  # For logit model

# Make binary classification
predictions_b <- ifelse(predictions_b_raw > 0.5, 1, 0)
predictions_c <- ifelse(predictions_c_raw > 0.5, 1, 0)
predictions_d <- ifelse(predictions_d_raw > 0.5, 1, 0)

# Get actual classes
actual_employed <- data$employed  # Assuming 'Employed' is the actual employment status

# Evaluate in-sample accuracy
lpm_b_accuracy <- sum(predictions_b == actual_employed) / length(actual_employed)
lpm_c_accuracy <- sum(predictions_c == actual_employed) / length(actual_employed)
lpm_d_accuracy <- sum(predictions_d == actual_employed) / length(actual_employed)

print(paste("The proportion of correctly assigned classes of model b is ", lpm_b_accuracy*100, "%"))
print(paste("The proportion of correctly assigned classes of model c is ", lpm_c_accuracy*100, "%"))
print(paste("The proportion of correctly assigned classes of model d is ", lpm_d_accuracy*100, "%"))

```
From the in-sample test, the proportion of correctly assgined classes for model b-d are all 87.5130944898387%.
Actually, the binary classification of the three models' prediction results are identical.

```{r}
if(identical(predictions_b, predictions_c) && identical(predictions_b, predictions_d))  {
  print("The binary classification of the three models' predictions are identical.")
} else {
  print("The binary classification of the three models' predictions are not identical.")
}
```

\newpage

## Question (h)

(1) Naïve Bayes Classifier
```{r}
nb_model <- naiveBayes(as.factor(employed) ~ age + age^2, data)
nb_predictions <- predict(nb_model, data, type = "class")
nb_accuracy <- sum(nb_predictions == actual_employed) / length(actual_employed)
print(paste("In-sample accuracy for Naive Bayes Classifier is ", nb_accuracy))
```
(2) Linear Discriminant Analysis
```{r}
lda_model <- lda(as.factor(employed) ~ age + I(`age`^2), data)
lda_predictions <- predict(lda_model, data)$class
lda_accuracy <- sum(lda_predictions == actual_employed) / length(actual_employed)
print(paste("In-sample accuracy for Linear Discriminant Analysis is ", lda_accuracy))
```
(3) Quadratic Discriminant Analysis
```{r}
qda_model <- qda(as.factor(employed) ~ age + I(`age`^2), data)
qda_predictions <- predict(qda_model, data)$class
qda_accuracy <- sum(qda_predictions == actual_employed) / length(actual_employed)
print(paste("In-sample accuracy for Quadratic Discriminant Analysis is ", qda_accuracy))
```
(4) Decision trees
```{r}
tree_model <- tree(as.factor(employed) ~ age + I(`age`^2), data)
tree_predictions <- predict(tree_model, data, type = "class")
tree_accuracy <- sum(tree_predictions == actual_employed) / length(actual_employed)
print(paste("In-sample accuracy for Decision Tree is ", tree_accuracy))
```
(5) Random forests
```{r}
set.seed(123) # Setting a seed for reproducibility
rf_model <- randomForest(as.factor(employed) ~ age + I(`age`^2), data, num.trees= 100) 
rf_predictions <- predict(rf_model, data)
rf_accuracy <- sum(rf_predictions == actual_employed) / length(actual_employed)
print(paste("In-sample accuracy for Random Forest (100 trees) is ", rf_accuracy))
```
(6) K-Nearest Neighbours
```{r}
# First scale the data since kNN is sensitive to the scale of the data
modified_data <- data %>% 
  mutate(square_age = age*age)
scaled_data <- scale(modified_data[, c("age", "square_age")])
# Define the number of neighbors
k <- 5
knn_predictions <- knn(train = scaled_data, test = scaled_data, cl = as.factor(data$employed), k = k)
knn_accuracy <- sum(knn_predictions == actual_employed) / length(actual_employed)
print(paste("In-sample accuracy for KNN (k=5, 5 nearest neighbours) is ", knn_accuracy))
```

--END--

